# Breaking Bias: An Approach to Quantify and Reduce Bias of LLMs in Multiple-Choice Answering

Welcome to our **NLP Course Project**, **Breaking Bias**, which explores methods to identify, quantify, and mitigate biases in Large Language Models (LLMs). We focus on biases that arise in multiple-choice scenarios, such as consistently favoring option "A" or always preferring earlier options.

This repository provides the code, datasets, trained models, and instructions to reproduce our experiments and further explore de-biasing techniques.

---

## Table of Contents

- [Project Overview](#project-overview)
- [Repository Structure](#repository-structure)
  - [mmlu01](#mmlu01)
  - [mmlu02](#mmlu02)
  - [mmlu03](#mmlu03)
  - [out_dir (Trained Models)](#out_dir-trained-models)
  - [logs (Training Logs)](#logs-training-logs)
  - [prompt_tuning](#prompt_tuning)
  - [COT (Chain-of-thought prompting)](#cot-chain-of-thought-prompting)
  - [plots](#plots)
  - [Scripts](#scripts)
- [Getting Started](#getting-started)
- [Experiment Setup](#experiment-setup)
- [Contact](#contact)
- [Acknowledgements](#acknowledgements)

---

## Project Overview

Modern LLMs can exhibit intrinsic biases, especially in multiple-choice question (MCQ) settings. These biases often appear as:
- **Option token bias**: Always favoring a particular option label (e.g., “A”, “B”, “C”, or “D”).
- **Position bias**: Preferring answers presented earlier or in a specific position within the options.

To address these issues, we perform targeted data augmentations, develop training setups (including permutations of question–answer pairs), and evaluate a variety of models—both baseline and finetuned—to quantify and reduce such biases. Our primary datasets and experiments revolve around the **MMLU** benchmark, but the methods and insights generalize to other multiple-choice tasks.

---

## Repository Structure

Below is an overview of the key directories and their contents:

### mmlu01
- **trainset / valset**: Original train and validation sets for experiment1.
- **permuted_train_set_8k / permuted_val_set_8k**: Permuted training and validation sets with 500 questions and 16 permutations per question (8k total).
- **permuted_train_set_16k / permuted_val_set_16k**: Expanded version of the above, totaling 16k questions.
- **permuted_train_set_32k / permuted_val_set_32k**: Further expanded sets, totaling 32k questions.
- **varying_option**: Test sets created by swapping the ground-truth label among A, B, C, D (4 permutations).
- **varying_position**: Test sets created by changing the position of the ground-truth answer (4 permutations).

#### In-Domain (ID) Subjects
- `testset`
- `permuted_testset_8k`
- `permuted_testset_16k`
- `permuted_testset_32k`

#### Out-of-Domain (OOD) Subjects
- `professional_law`
- `prehistory`
- `philosophy`
- `high_school_mathematics`
- `conceptual_physics`
- `college_medicine`
- `abstract_algebra`

### mmlu02
- **Inference results** generated by various baseline and finetuned models on the test datasets from `mmlu01 (varying_option and varying_position)`.
- **Baseline Models**: 
  - `llama-7b`, `llama-2-7b`, `llama-2-13b`, `llama-3-8b`,  
    `mistral-7b`, `mistral-7b-instruct`, `vicuna-7b`,  
    `vicuna-13b`, `gemma-2b`, `gemma-2b-it`, `gemma-7b-it`, `gemma-7b`.
- **Experiment Models**:
  - *Experiment 1*: `llama-2-7b-lora-tuned`, `llama-7b-lora-tuned`, `gemma-2b-it-lora-tuned`, `gemma-7b-it-lora-tuned`
  - *Experiment 2*: `llama-2-7b-lora-tuned-per-8k`, `llama-7b-lora-tuned-per-8k`, `gemma-2b-it-lora-tuned-per-8k`
  - *Experiment 3*: `llama-2-7b-lora-tuned-per-16k`, `llama-7b-lora-tuned-per-16k`, `gemma-2b-it-lora-tuned-per-16k`
  - *Experiment 4*: `llama-2-7b-lora-tuned-per-32k`
  - Other: `llama-7b-1shot-cot`

### mmlu03
- **Evaluation results** and comparative analysis plots for the inference outputs in `mmlu02`.
- **Metrics**: Error bins, PPA (permutation position accuracy) scores, recall imbalance, and incorrect_likelihood across in-domain and out-of-domain test datasets.
- Results are stored as CSVs in `mmlu03/<model-name>/...`.

### out_dir (Trained Models)
- Contains the **trained models** from the above experiments.  
- These can be loaded for further finetuning or inference.

### logs (Training Logs)
- Stores training logs from the LoRA-tuning process and any relevant training events.

### prompt_tuning
- Scripts related to prompt-based tuning strategies.

### COT (Chain-of-thought prompting)
- Scripts that implement and experiment with chain-of-thought prompting.

### plots
- Contains scripts for plotting the evaluation metrics produced in `mmlu03`.

---

## Scripts

1. **`Evaluation.ipynb`**  
   - Generates evaluation metrics and stores the results as CSV files.

2. **`Inference.ipynb`**  
   - Loads baseline or finetuned models to generate inferences on test sets.  
   - Provides commented-out code sections for:
     - Loading pretrained models from Hugging Face or other SDKs.
     - Loading locally trained models from `out_dir`.

3. **`Create_datasets.ipynb`**  
   - Builds or modifies the datasets in `mmlu01`, including permutations.

4. **`Error_analysis.ipynb`**  
   - Identifies specific MCQs where baseline or trained models fail.  
   - Results are stored in:
     - `mmlu/<model-name>/varying_option/error_analysis_cases`
     - `mmlu/<model-name>/varying_position/error_analysis_cases`

5. **`lora_tuning.ipynb`**  
   - Scripts to train or finetune models using LoRA techniques.

---

## Getting Started

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/breaking-bias-llm.git
2. **Create and Activate Conda Environment**:
    Create a conda virtual environment and activate it:
    ```bash
    conda create -n myenv python=3.9
    conda activate myenv
3. **Clone the repository**:
    Install the required dependencies using pip:
    ```bash
    pip install -r requirements.txt

## Experiment Setup

This section outlines the experimental setup for quantifying and reducing bias in LLMs through various training and evaluation strategies:

### 1. Original Tuning (Without Permutations)

- **Dataset Used**: 
  - `trainset` for training 
  - `valset` for validation

- **Procedure**:  
  Train your model directly using the original, unmodified dataset (`trainset`). Evaluate its performance periodically using the validation set (`valset`) to monitor for overfitting.

---

### 2. Permutation-based Tuning

Permutation experiments aim to reduce biases by training on datasets where the ordering of multiple-choice options is systematically varied.

- **Datasets Available**:
  - `permuted_train_set_8k` / `permuted_val_set_8k`
  - `permuted_train_set_16k` / `permuted_val_set_16k`
  - `permuted_train_set_32k` / `permuted_val_set_32k`

- **Procedure**:  
  Choose one of the permutation sets based on your computational resources. Train your model on the selected permuted training set, and evaluate it using the corresponding permuted validation set.

---

### 3. LoRA Tuning (Fine-tuning)

- **Notebook**: `lora_tuning.ipynb`

- **Procedure**:  
  - Adjust the dataset paths to point to your desired training and validation datasets.
  - Set your hyperparameters (e.g., batch size, learning rate, epochs) within the notebook.
  - Execute the notebook to initiate LoRA-based fine-tuning on your selected LLM architecture.

---

### 4. Evaluation of LLM Bias

- **Notebook**: `Evaluation.ipynb`

- **Datasets**:
  - **In-Domain (ID)** datasets (e.g., science, mathematics, etc.)
  - **Out-of-Domain (OOD)** datasets (e.g., law, philosophy, prehistory, etc.)

- **Procedure**:  
  - Run the evaluation script to quantify biases using predefined evaluation metrics:
    - Error bins
    - PPA (Permutation Position Accuracy) scores
    - Recall imbalance
    - Incorrect likelihood
  - Results will be saved as `.csv` files in the respective folders in `mmlu03`.
  - Visualize and compare the evaluation results using scripts provided in the `plots` directory.

--- 
## System Requirements

For optimal performance during training and inference, ensure your system meets the following specifications:

- **GPU**: NVIDIA L4 GPU or equivalent CUDA-compatible GPU
- **RAM**: Minimum of 32 GB recommended
- **Storage**: Sufficient disk space for datasets, trained models, and logs (at least 100 GB recommended)

---

## Acknowledgements

We gratefully acknowledge the creators of the MMLU benchmark datasets and all contributors of the various open-source Large Language Models (LLMs) utilized in this project. Their invaluable contributions have significantly advanced our research and made this work possible.

